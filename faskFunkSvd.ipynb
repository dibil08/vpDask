{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0303baa7afefb035e1550d302a745de05b987cfd3e399986f3eb98c2da1860a3d",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tomaz\\anaconda3\\lib\\site-packages\\distributed\\node.py:151: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 50388 instead\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Client: 'tcp://127.0.0.1:50391' processes=3 threads=6, memory=12.00 GB>",
      "text/html": "<table style=\"border: 2px solid white;\">\n<tr>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Client</h3>\n<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n  <li><b>Scheduler: </b>tcp://127.0.0.1:50391</li>\n  <li><b>Dashboard: </b><a href='http://127.0.0.1:50388/status' target='_blank'>http://127.0.0.1:50388/status</a></li>\n</ul>\n</td>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Cluster</h3>\n<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n  <li><b>Workers: </b>3</li>\n  <li><b>Cores: </b>6</li>\n  <li><b>Memory: </b>12.00 GB</li>\n</ul>\n</td>\n</tr>\n</table>"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 item   user   rating\n",
       "npartitions=51                       \n",
       "                int64  int64  float64\n",
       "                  ...    ...      ...\n",
       "...               ...    ...      ...\n",
       "                  ...    ...      ...\n",
       "                  ...    ...      ...\n",
       "Dask Name: read-parquet, 51 tasks"
      ],
      "text/html": "<div><strong>Dask DataFrame Structure:</strong></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item</th>\n      <th>user</th>\n      <th>rating</th>\n    </tr>\n    <tr>\n      <th>npartitions=51</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th></th>\n      <td>int64</td>\n      <td>int64</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<div>Dask Name: read-parquet, 51 tasks</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import multiprocessing\n",
    "import  numpy as np\n",
    "import pandas as pd\n",
    "from dask import array as da\n",
    "\n",
    "dask.config.set({\"distributed.worker.memory.terminate\": False})\n",
    "dask.config.set({\"distributed.scheduler.allowed-failures\": 50})\n",
    "outputFile=\"allCSV100mb.txt\"\n",
    "#parquetToRead=\"parquet100MBall_csv_files\"\n",
    "\n",
    "parquetToRead=\"parquet100MBall_csv_files\"\n",
    "cluster = LocalCluster(memory_limit='4GB')\n",
    "#client = Client(processes=False)\n",
    "#client = Client(cluster, asynchronous=True)\n",
    "client = Client(cluster)\n",
    "display(client)\n",
    "\n",
    "data=dd.read_parquet(parquetToRead, names=[\"user\", \"item\", \"rating\"],indexstr=False,gather_statistics=True)\n",
    "\n",
    "#csvToRead=\"smallcsv/10MBratings_Toys_and_Games.csv\"\n",
    "#data=dd.read_csv(csvToRead, names=[\"item\", \"user\", \"rating\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 46 s\n43531850\n15167257\n"
     ]
    }
   ],
   "source": [
    "# Get len of users and item\n",
    "# +1 couse index start at 0\n",
    "%time userLen,itemLen =dd.compute(data.user.max()+1,data.item.max()+1)\n",
    "print(userLen)\n",
    "print(itemLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.1 0.1 0.1 ... 0.1 0.1 0.1]\n [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n ...\n [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n [0.1 0.1 0.1 ... 0.1 0.1 0.1]]\n[[0.1 0.1 0.1 ... 0.1 0.1 0.1]\n [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n ...\n [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n [0.1 0.1 0.1 ... 0.1 0.1 0.1]\n [0.1 0.1 0.1 ... 0.1 0.1 0.1]]\n"
     ]
    }
   ],
   "source": [
    "# create latent factor matrixes for user and item\n",
    "features=40\n",
    "\n",
    "user_lf=da.full((userLen,features),0.1).compute()\n",
    "item_lf=da.full((itemLen,features),0.1).compute()\n",
    "print(user_lf)\n",
    "print(item_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 1min 45s\n",
      "Wall time: 3min 45s\n",
      "Wall time: 29.8 s\n",
      "global user mean: 4.075073563511038\n",
      "global item mean: 4.181384720569861\n",
      "global rating: 4.232190208636595\n"
     ]
    }
   ],
   "source": [
    "#Compute global averages\n",
    "\n",
    "%time globalItemMean = data.groupby(\"item\").rating.mean().mean().compute()\n",
    "%time globalUserMean = data.groupby(\"user\").rating.mean().mean().compute()\n",
    "%time globalRatingMean = data.rating.mean().compute()\n",
    "print(\"global user mean: {}\".format(globalUserMean))\n",
    "print(\"global item mean: {}\".format(globalItemMean))\n",
    "print(\"global rating: {}\".format(globalRatingMean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=25\n",
    "def computeUserAverageOffset(row):\n",
    "    return globalItemMean-(((k*globalUserMean)+row['rating']['mean'])/(row['rating']['count']+k))\n",
    "\n",
    "def computeItemAverage(row):\n",
    "    return globalUserMean-((k*globalItemMean)+row['rating']['mean'])/(row['rating']['count']+k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "user\n0           1.706213\n1           0.364355\n2           1.764327\n3           1.712673\n4           1.752366\n              ...   \n43531845    0.070737\n43531846    0.371131\n43531847    0.070737\n43531848    0.070737\n43531849    0.070737\nName: betterMean, Length: 43531850, dtype: float64\nitem\n0          -0.137796\n1           0.163123\n2          -0.137796\n3           3.359665\n4           3.470024\n              ...   \n15167252    3.678200\n15167253    3.854606\n15167254    3.211409\n15167255    3.859777\n15167256    0.018236\nName: betterMean, Length: 15167257, dtype: float64\nWall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "userOffset=data.groupby('user')\\\n",
    "                    .agg({'rating': ['mean', 'count']})\\\n",
    "                    .map_partitions(computeUserAverageOffset, meta=(\"betterMean\",float)\\\n",
    "                    )\\\n",
    "                    .compute()\n",
    "itemOffset=data.groupby('item')\\\n",
    "                    .agg({'rating': ['mean', 'count']})\\\n",
    "                    .map_partitions(computeItemAverage, meta=(\"betterMean\",float))\\\n",
    "                    .compute()\n",
    "print(userOffset)\n",
    "print(itemOffset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 4.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bias_user=da.from_array(userOffset).compute()\n",
    "bias_item=da.from_array(itemOffset).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from dask import delayed\n",
    "# https://github.com/gbolmier/funk-svd/blob/master/funk_svd/fast_methods.py\n",
    "\n",
    "def run_epoch(X, bu, bi, pu, qi, global_mean, n_factors, lr, reg,new_bu,new_bi,new_pu,new_qi):\n",
    "    \"\"\"Runs an epoch, updating model weights (pu, qi, bu, bi).\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.array\n",
    "        Training set.\n",
    "    bu : numpy.array\n",
    "        User biases vector.\n",
    "    bi : numpy.array\n",
    "        Item biases vector.\n",
    "    pu : numpy.array\n",
    "        User latent factors matrix.\n",
    "    qi : numpy.array\n",
    "        Item latent factors matrix.\n",
    "    global_mean : float\n",
    "        Ratings arithmetic mean.\n",
    "    n_factors : int\n",
    "        Number of latent factors.\n",
    "    lr : float\n",
    "        Learning rate.\n",
    "    reg : float\n",
    "        L2 regularization factor.\n",
    "    Returns:\n",
    "    --------\n",
    "    bu : numpy.array\n",
    "        User biases vector.\n",
    "    bi : numpy.array\n",
    "        Item biases vector.\n",
    "    pu : numpy.array\n",
    "        User latent factors matrix.\n",
    "    qi : numpy.array\n",
    "        Item latent factors matrix.\n",
    "    \"\"\"\n",
    "    for i in range(X.shape[0]):\n",
    "        user, item, rating = int(X[i, 0]), int(X[i, 1]), X[i, 2]\n",
    "\n",
    "        # Predict current rating\n",
    "        pred = global_mean + bu[user] + bi[item]\n",
    "\n",
    "        for factor in range(n_factors):\n",
    "            pred += pu[user, factor] * qi[item, factor]\n",
    "\n",
    "        err = rating - pred\n",
    "\n",
    "        # Update biases\n",
    "        new_bu[user] += lr * (err - reg * bu[user])\n",
    "        new_bi[item] += lr * (err - reg * bi[item])\n",
    "\n",
    "\n",
    "        # Update latent factors\n",
    "        for factor in range(n_factors):\n",
    "            puf = pu[user, factor]\n",
    "            qif = qi[item, factor]\n",
    "\n",
    "            new_pu[user, factor] += lr * (err * qif - reg * puf)\n",
    "            new_qi[item, factor] += lr * (err * puf - reg * qif)\n",
    "    return np.array([new_bu, new_bi, new_pu, new_qi],dtype=object)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_metrics(X_val, bu, bi, pu, qi, global_mean, n_factors,residuals):\n",
    "    \"\"\"Computes validation metrics (loss, rmse, and mae).\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_val : numpy.array\n",
    "        Validation set.\n",
    "    bu : numpy.array\n",
    "        User biases vector.\n",
    "    bi : numpy.array\n",
    "        Item biases vector.\n",
    "    pu : numpy.array\n",
    "        User latent factors matrix.\n",
    "    qi : numpy.array\n",
    "        Item latent factors matrix.\n",
    "    global_mean : float\n",
    "        Ratings arithmetic mean.\n",
    "    n_factors : int\n",
    "        Number of latent factors.\n",
    "    Returns\n",
    "    -------\n",
    "    loss, rmse, mae : tuple of floats\n",
    "        Validation loss, rmse and mae.\n",
    "    \"\"\"\n",
    "    #residuals =np.arange(X_val.shape[0])\n",
    "\n",
    "    for i in range(X_val.shape[0]):\n",
    "        user, item, rating = int(X_val[i, 0]), int(X_val[i, 1]), X_val[i, 2]\n",
    "        pred = global_mean\n",
    "\n",
    "        if user > -1:\n",
    "            pred += bu[user]\n",
    "\n",
    "        if item > -1:\n",
    "            pred += bi[item]\n",
    "\n",
    "        if (user > -1) and (item > -1):\n",
    "            for factor in range(n_factors):\n",
    "                pred += pu[user, factor] * qi[item, factor]\n",
    "\n",
    "        residuals[i]=(rating - pred)\n",
    "   \n",
    "    return residuals[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "233055327\n",
      "dask.array<concatenate, shape=(163138706, 3), dtype=float64, chunksize=(3634424, 3), chunktype=numpy.ndarray>\n",
      "dask.array<concatenate, shape=(69916621, 3), dtype=float64, chunksize=(1557611, 3), chunktype=numpy.ndarray>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dask.array<concatenate, shape=(163138706, 3), dtype=float64, chunksize=(3634424, 3), chunktype=numpy.ndarray>"
      ],
      "text/html": "<table>\n<tr>\n<td>\n<table>\n  <thead>\n    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n  </thead>\n  <tbody>\n    <tr><th> Bytes </th><td> 3.92 GB </td> <td> 87.23 MB </td></tr>\n    <tr><th> Shape </th><td> (163138706, 3) </td> <td> (3634424, 3) </td></tr>\n    <tr><th> Count </th><td> 408 Tasks </td><td> 51 Chunks </td></tr>\n    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n  </tbody>\n</table>\n</td>\n<td>\n<svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n\n  <!-- Horizontal lines -->\n  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n  <line x1=\"0\" y1=\"5\" x2=\"25\" y2=\"5\" />\n  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n  <line x1=\"0\" y1=\"19\" x2=\"25\" y2=\"19\" />\n  <line x1=\"0\" y1=\"24\" x2=\"25\" y2=\"24\" />\n  <line x1=\"0\" y1=\"32\" x2=\"25\" y2=\"32\" />\n  <line x1=\"0\" y1=\"39\" x2=\"25\" y2=\"39\" />\n  <line x1=\"0\" y1=\"44\" x2=\"25\" y2=\"44\" />\n  <line x1=\"0\" y1=\"51\" x2=\"25\" y2=\"51\" />\n  <line x1=\"0\" y1=\"58\" x2=\"25\" y2=\"58\" />\n  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n  <line x1=\"0\" y1=\"70\" x2=\"25\" y2=\"70\" />\n  <line x1=\"0\" y1=\"77\" x2=\"25\" y2=\"77\" />\n  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n  <line x1=\"0\" y1=\"89\" x2=\"25\" y2=\"89\" />\n  <line x1=\"0\" y1=\"96\" x2=\"25\" y2=\"96\" />\n  <line x1=\"0\" y1=\"100\" x2=\"25\" y2=\"100\" />\n  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n  <line x1=\"0\" y1=\"114\" x2=\"25\" y2=\"114\" />\n  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n\n  <!-- Vertical lines -->\n  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n\n  <!-- Colored Rectangle -->\n  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n\n  <!-- Text -->\n  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">163138706</text>\n</svg>\n</td>\n</tr>\n</table>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import dask_ml\n",
    "print(len(data.index))\n",
    "train_data, test_data=dask_ml.model_selection.train_test_split(\\\n",
    "                                                data.to_dask_array(lengths=True),\n",
    "                                                test_size=0.3)\n",
    "print(train_data)\n",
    "print(test_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1557611\n"
     ]
    }
   ],
   "source": [
    "max_chunk_size=max(test_data.compute_chunk_sizes().chunks[0])\n",
    "print(max_chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "computing results\n",
      "epoch:0\t\tloss:4.939606219525349\t\trmse:2.222522490218119\t\tmae:2.222522490218119\t\ttrain time45.62499976158142\t\teval time4.800001859664917\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "import time\n",
    "epochs=40\n",
    "lrate = 0.001\n",
    "#L2 regularization factor / weight decay\n",
    "reg=0.01\n",
    "user_lf=np.array(user_lf)\n",
    "item_lf=np.array(item_lf)\n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "    remote_user_lf=client.scatter(user_lf)\n",
    "    remote_item_lf=client.scatter(item_lf)\n",
    "    res = da.map_blocks(\\\n",
    "                                        run_epoch,\\\n",
    "                                        train_data,\\\n",
    "                                        bias_user, \\\n",
    "                                        bias_item,\\\n",
    "                                        remote_user_lf,\\\n",
    "                                        remote_item_lf,\\\n",
    "                                        globalRatingMean,\\\n",
    "                                        features,\\\n",
    "                                        lrate,\\\n",
    "                                        reg,\\\n",
    "                                        np.zeros(shape=bias_user.shape),\\\n",
    "                                        np.zeros(shape=bias_item.shape),\\\n",
    "                                        np.zeros(shape=user_lf.shape),\\\n",
    "                                        np.zeros(shape=item_lf.shape),\\\n",
    "                                        dtype=object)\\\n",
    "                                    .compute()\n",
    "    new_bu=res[:,0]\n",
    "    new_bi=res[:,1]\n",
    "    new_pu=res[:,2]\n",
    "    new_qi=res[:,3]\n",
    "    bias_user += sum(new_bu)/len(new_bu)\n",
    "    bias_item += sum(new_bi)/len(new_bi)\n",
    "    user_lf += sum(new_pu)/len(new_pu)\n",
    "    item_lf += sum(new_qi)/len(new_qi)\n",
    "    end_time=time.time()\n",
    "    print(\"computing results\")\n",
    "    result_time = time.time()\n",
    "    residuals= da.map_blocks(compute_val_metrics,test_data,bias_user,bias_item,user_lf,item_lf,globalRatingMean,features,np.full(max_chunk_size, fill_value=np.nan),dtype=object).compute()\n",
    "    residuals=np.ma.array(residuals, mask=np.isnan(residuals))\n",
    "    loss = np.square(residuals.mean())\n",
    "    rmse = np.sqrt(loss)\n",
    "    mae = np.absolute(residuals.mean())\n",
    "    t=result_time-start_time\n",
    "    print(\"epoch:{}\\t\\tloss:{}\\t\\trmse:{}\\t\\tmae:{}\\t\\ttrain time{}\\t\\teval time{}\".format(i,loss,rmse,mae,t,time.time()-result_time))\n",
    "    file_object = open(outputFile, 'a')\n",
    "    file_object.write(\"epoch:{}\\t\\tloss:{}\\t\\trmse:{}\\t\\tmae:{}\\t\\ttrain time{}\\t\\teval time{}\\n\".format(i,loss,rmse,mae,t,time.time()-result_time))\n",
    "    file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch:0\t\tloss:38.12679057918392\t\trmse:6.1746895127758386\t\tmae:3.832001188894558\t\ttime370.52927470207214"
   ]
  }
 ]
}